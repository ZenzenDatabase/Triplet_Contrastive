{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "523b3533",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47854a23",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def load_mnist():\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "    dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "    data = torch.stack([d[0] for d in dataset])  # [N, 1, 28, 28]\n",
    "    labels = torch.tensor([d[1] for d in dataset])\n",
    "    return data.to(device), labels.to(device)\n",
    "\n",
    "def contrastive_loss(x1, x2, label, margin=1.0):\n",
    "    dist = F.pairwise_distance(x1, x2)\n",
    "    return (label * dist.pow(2) + (1 - label) * F.relu(margin - dist).pow(2)).mean()\n",
    "\n",
    "def triplet_loss(anchor, pos, neg, margin=1.0):\n",
    "    d_pos = F.pairwise_distance(anchor, pos)\n",
    "    d_neg = F.pairwise_distance(anchor, neg)\n",
    "    return F.relu(d_pos - d_neg + margin).mean()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1619917",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNetEmbedder(nn.Module):\n",
    "    \"\"\"\n",
    "    A convolutional neural network for learning embeddings.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_channels=3, input_height=32, input_width=32):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, 32, 3, 1, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, 3, 1, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "        )\n",
    "\n",
    "        # Compute correct flattened size using actual height & width\n",
    "        with torch.no_grad():\n",
    "            # Create a dummy tensor to pass through the feature extractor\n",
    "            dummy = torch.zeros(1, input_channels, input_height, input_width)\n",
    "            out = self.features(dummy)\n",
    "            self.flattened_size = out.view(1, -1).size(1)\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(self.flattened_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64), # Output embedding dimension is 64\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the network.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The normalized embedding.\n",
    "        \"\"\"\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten the tensor\n",
    "        x = self.fc(x)\n",
    "        return F.normalize(x, dim=-1) # L2 normalize the embeddings\n",
    "\n",
    "# ----------------------\n",
    "# Losses\n",
    "# ----------------------\n",
    "def contrastive_loss(x1, x2, label, margin=1.0):\n",
    "    \"\"\"\n",
    "    Computes the contrastive loss.\n",
    "\n",
    "    Args:\n",
    "        x1 (torch.Tensor): Embedding of the first sample.\n",
    "        x2 (torch.Tensor): Embedding of the second sample.\n",
    "        label (torch.Tensor): Label indicating if the pair is similar (1) or dissimilar (0).\n",
    "        margin (float): Margin for dissimilar pairs.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The computed contrastive loss.\n",
    "    \"\"\"\n",
    "    dist = F.pairwise_distance(x1, x2)\n",
    "    # loss = label * dist^2 + (1 - label) * max(0, margin - dist)^2\n",
    "    return (label * dist.pow(2) + (1 - label) * F.relu(margin - dist).pow(2)).mean()\n",
    "\n",
    "def triplet_loss(anchor, pos, neg, margin=1.0):\n",
    "    \"\"\"\n",
    "    Computes the triplet loss.\n",
    "\n",
    "    Args:\n",
    "        anchor (torch.Tensor): Embedding of the anchor sample.\n",
    "        pos (torch.Tensor): Embedding of the positive sample.\n",
    "        neg (torch.Tensor): Embedding of the negative sample.\n",
    "        margin (float): Margin for the triplet loss.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The computed triplet loss.\n",
    "    \"\"\"\n",
    "    d_pos = F.pairwise_distance(anchor, pos)\n",
    "    d_neg = F.pairwise_distance(anchor, neg)\n",
    "    # loss = max(0, d_pos - d_neg + margin)\n",
    "    return F.relu(d_pos - d_neg + margin).mean()\n",
    "\n",
    "# ----------------------\n",
    "# Training Loop\n",
    "# ----------------------\n",
    "def train_embedding_model(data, labels, loss_type='triplet', epochs=20, batch_size=256):\n",
    "    \"\"\"\n",
    "    Trains the embedding model using either contrastive or triplet loss.\n",
    "\n",
    "    Args:\n",
    "        data (torch.Tensor): The input data.\n",
    "        labels (torch.Tensor): The corresponding labels.\n",
    "        loss_type (str): Type of loss to use ('triplet' or 'contrastive').\n",
    "        epochs (int): Number of training epochs.\n",
    "        batch_size (int): Batch size for training.\n",
    "\n",
    "    Returns:\n",
    "        nn.Module: The trained embedding model in evaluation mode.\n",
    "    \"\"\"\n",
    "    channels, height, width = data.size(1), data.size(2), data.size(3)\n",
    "    # Fix: Pass input_height and input_width explicitly\n",
    "    model = ConvNetEmbedder(input_channels=channels, input_height=height, input_width=width).to(device).train()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    n = len(data)\n",
    "\n",
    "    print(f\"Starting training with {loss_type} loss for {epochs} epochs...\")\n",
    "    for epoch in range(epochs):\n",
    "        perm = torch.randperm(n)\n",
    "        total_loss = 0.0\n",
    "        num_batches = 0\n",
    "        for i in range(0, n, batch_size):\n",
    "            batch_idx = perm[i:i + batch_size]\n",
    "            x_batch, y_batch = data[batch_idx], labels[batch_idx]\n",
    "            \n",
    "            # Ensure the model is in training mode during this loop\n",
    "            model.train() \n",
    "            x_embed = model(x_batch)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            current_loss = torch.tensor(0.0).to(device) # Initialize current_loss for the batch\n",
    "\n",
    "            if loss_type == 'triplet':\n",
    "                anchors, positives, negatives = [], [], []\n",
    "                # Collect triplets within the current batch\n",
    "                for j in range(len(x_batch)):\n",
    "                    anchor_label = y_batch[j].item()\n",
    "                    \n",
    "                    # Find potential positive samples within the batch\n",
    "                    pos_candidates_in_batch = (y_batch == anchor_label).nonzero(as_tuple=True)[0]\n",
    "                    # Exclude the anchor itself\n",
    "                    pos_candidates_in_batch = pos_candidates_in_batch[pos_candidates_in_batch != j]\n",
    "\n",
    "                    # Find potential negative samples within the batch\n",
    "                    neg_candidates_in_batch = (y_batch != anchor_label).nonzero(as_tuple=True)[0]\n",
    "\n",
    "                    if len(pos_candidates_in_batch) > 0 and len(neg_candidates_in_batch) > 0:\n",
    "                        pos_j = random.choice(pos_candidates_in_batch).item()\n",
    "                        neg_j = random.choice(neg_candidates_in_batch).item()\n",
    "\n",
    "                        anchors.append(x_embed[j])\n",
    "                        positives.append(x_embed[pos_j])\n",
    "                        negatives.append(x_embed[neg_j])\n",
    "                \n",
    "                if anchors:\n",
    "                    current_loss = triplet_loss(torch.stack(anchors), torch.stack(positives), torch.stack(negatives))\n",
    "                else:\n",
    "                    # If no valid triplets found in batch, skip this batch for loss calculation\n",
    "                    continue\n",
    "\n",
    "            elif loss_type == 'contrastive':\n",
    "                a, b, l = [], [], []\n",
    "                for j in range(len(x_batch)):\n",
    "                    current_label = y_batch[j].item()\n",
    "                    \n",
    "                    # 50% chance for positive pair, 50% for negative\n",
    "                    if random.random() < 0.5: # Positive pair\n",
    "                        pos_candidates_in_batch = (y_batch == current_label).nonzero(as_tuple=True)[0]\n",
    "                        pos_candidates_in_batch = pos_candidates_in_batch[pos_candidates_in_batch != j]\n",
    "                        if len(pos_candidates_in_batch) > 0:\n",
    "                            k = random.choice(pos_candidates_in_batch).item()\n",
    "                            a.append(x_embed[j])\n",
    "                            b.append(x_embed[k])\n",
    "                            l.append(1.0)\n",
    "                    else: # Negative pair\n",
    "                        neg_candidates_in_batch = (y_batch != current_label).nonzero(as_tuple=True)[0]\n",
    "                        if len(neg_candidates_in_batch) > 0:\n",
    "                            k = random.choice(neg_candidates_in_batch).item()\n",
    "                            a.append(x_embed[j])\n",
    "                            b.append(x_embed[k])\n",
    "                            l.append(0.0)\n",
    "                \n",
    "                if a: # If pairs were generated\n",
    "                    current_loss = contrastive_loss(torch.stack(a), torch.stack(b), torch.tensor(l).to(device))\n",
    "                else:\n",
    "                    # If no valid pairs found in batch, skip this batch for loss calculation\n",
    "                    continue\n",
    "            \n",
    "            if current_loss > 0: # Only backpropagate if there's an actual loss\n",
    "                current_loss.backward()\n",
    "                optimizer.step()\n",
    "                total_loss += current_loss.item()\n",
    "                num_batches += 1\n",
    "        \n",
    "        avg_loss = total_loss / num_batches if num_batches > 0 else 0\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    return model.eval() # Return model in evaluation mode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f197bb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training with contrastive loss for 20 epochs...\n",
      "Epoch 1/20, Average Loss: 0.0575\n",
      "Epoch 2/20, Average Loss: 0.0191\n",
      "Epoch 3/20, Average Loss: 0.0136\n",
      "Epoch 4/20, Average Loss: 0.0099\n",
      "Epoch 5/20, Average Loss: 0.0087\n",
      "Epoch 6/20, Average Loss: 0.0075\n",
      "Epoch 7/20, Average Loss: 0.0068\n",
      "Epoch 8/20, Average Loss: 0.0063\n",
      "Epoch 9/20, Average Loss: 0.0057\n",
      "Epoch 10/20, Average Loss: 0.0053\n",
      "Epoch 11/20, Average Loss: 0.0049\n",
      "Epoch 12/20, Average Loss: 0.0047\n",
      "Epoch 13/20, Average Loss: 0.0034\n",
      "Epoch 14/20, Average Loss: 0.0043\n",
      "Epoch 15/20, Average Loss: 0.0035\n",
      "Epoch 16/20, Average Loss: 0.0029\n",
      "Epoch 17/20, Average Loss: 0.0031\n",
      "Epoch 18/20, Average Loss: 0.0024\n",
      "Epoch 19/20, Average Loss: 0.0024\n",
      "Epoch 20/20, Average Loss: 0.0044\n",
      "Starting training with triplet loss for 20 epochs...\n",
      "Epoch 1/20, Average Loss: 0.1056\n",
      "Epoch 2/20, Average Loss: 0.0283\n",
      "Epoch 3/20, Average Loss: 0.0200\n",
      "Epoch 4/20, Average Loss: 0.0154\n",
      "Epoch 5/20, Average Loss: 0.0131\n",
      "Epoch 6/20, Average Loss: 0.0110\n",
      "Epoch 7/20, Average Loss: 0.0089\n",
      "Epoch 8/20, Average Loss: 0.0090\n",
      "Epoch 9/20, Average Loss: 0.0090\n",
      "Epoch 10/20, Average Loss: 0.0074\n",
      "Epoch 11/20, Average Loss: 0.0061\n",
      "Epoch 12/20, Average Loss: 0.0057\n",
      "Epoch 13/20, Average Loss: 0.0048\n",
      "Epoch 14/20, Average Loss: 0.0056\n",
      "Epoch 15/20, Average Loss: 0.0052\n",
      "Epoch 16/20, Average Loss: 0.0042\n",
      "Epoch 17/20, Average Loss: 0.0042\n",
      "Epoch 18/20, Average Loss: 0.0041\n",
      "Epoch 19/20, Average Loss: 0.0035\n",
      "Epoch 20/20, Average Loss: 0.0030\n"
     ]
    }
   ],
   "source": [
    "data1, labels1 = load_mnist()\n",
    "\n",
    "model_con1 = train_embedding_model(data1, labels1, loss_type='contrastive')\n",
    "data_c1_list = []\n",
    "with torch.no_grad():\n",
    "    for i in range(0, len(data1), 256): \n",
    "        batch_data = data1[i:i+256]\n",
    "        data_c1_list.append(model_con1(batch_data).detach().cpu())\n",
    "data_c1 = torch.cat(data_c1_list).to(device) \n",
    "\n",
    "model_tri1 = train_embedding_model(data1, labels1, loss_type='triplet')\n",
    "data_t1_list = []\n",
    "with torch.no_grad():\n",
    "    for i in range(0, len(data1), 256):\n",
    "        batch_data = data1[i:i+256]\n",
    "        data_t1_list.append(model_tri1(batch_data).detach().cpu())\n",
    "data_t1 = torch.cat(data_t1_list).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f936b06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0.3494914770126343, 1: 0.16978716850280762, 2: 0.3682921826839447, 3: 0.33302924036979675, 4: 0.3112839162349701, 5: 0.36207902431488037, 6: 0.3122705817222595, 7: 0.2790134847164154, 8: 0.3301646411418915, 9: 0.2867642045021057}\n",
      "{0: 0.000799543980974704, 1: 0.0015676108887419105, 2: 0.003525828942656517, 3: 0.0022698622196912766, 4: 0.002072482369840145, 5: 0.0033548111096024513, 6: 0.0036530648358166218, 7: 0.003849779022857547, 8: 0.0026101875118911266, 9: 0.0059341671876609325}\n",
      "{0: 0.006985311396420002, 1: 0.00323380995541811, 2: 0.010066665709018707, 3: 0.004289098549634218, 4: 0.005846464075148106, 5: 0.0072196065448224545, 6: 0.005104381125420332, 7: 0.0041129146702587605, 8: 0.003503313986584544, 9: 0.00861962977796793}\n"
     ]
    }
   ],
   "source": [
    "from metric import compute_intra_class_variance, compute_inter_class_variance\n",
    "\n",
    "variances_original = compute_intra_class_variance(data1, labels1)\n",
    "print(variances_original)\n",
    "\n",
    "variances_contrastive = compute_intra_class_variance(data_c1, labels)\n",
    "print(variances_contrastive)\n",
    "\n",
    "variances_triplet = compute_intra_class_variance(data_t1, labels)\n",
    "print(variances_triplet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5b29b67f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intra-class variance - Mean (contrastive): 0.0030, Std: 0.0014\n",
      "Intra-class variance - Mean (triplet): 0.0059, Std: 0.0022\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "intra_values = list(variances_contrastive.values())\n",
    "intra_mean = np.mean(intra_values)\n",
    "intra_std = np.std(intra_values)\n",
    "print(f\"Intra-class variance - Mean (contrastive): {intra_mean:.4f}, Std: {intra_std:.4f}\")\n",
    "intra_values = list(variances_triplet.values())\n",
    "intra_mean = np.mean(intra_values)\n",
    "intra_std = np.std(intra_values)\n",
    "print(f\"Intra-class variance - Mean (triplet): {intra_mean:.4f}, Std: {intra_std:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ff19d40d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inter-class variance - Mean (Contrastive): 1.0347, Std: 0.0800\n",
      "Inter-class variance - Mean (Triplet): 1.4840, Std: 0.0685\n"
     ]
    }
   ],
   "source": [
    "inter_mean, inter_std = compute_inter_class_variance(data_c1, labels)\n",
    "print(f\"Inter-class variance - Mean (Contrastive): {inter_mean:.4f}, Std: {inter_std:.4f}\")\n",
    "inter_mean, inter_std = compute_inter_class_variance(data_t1, labels)\n",
    "print(f\"Inter-class variance - Mean (Triplet): {inter_mean:.4f}, Std: {inter_std:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e7fe6a26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0064, 0.004692250000000001)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.0800*0.0800, 0.0685*0.0685"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1060f84",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "py312"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
